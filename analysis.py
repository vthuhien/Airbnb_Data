# -*- coding: utf-8 -*-
"""Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/vthuhien/Airbnb_Data/blob/main/Analysis.ipynb

## About This Data
This dataset is downloaded from Kaggle: https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata?resource=download <br/>
We also up the file zip of data [here](https://github.com/vthuhien/Airbnb_Data/blob/main/Airbnb_Open_Data.7z) in Github. And if you want to see directly or are lazy to unzip this file, [here](https://drive.google.com/file/d/11k6K0WwlkmGl3pg82v29taFKAae7qSIh/view?usp=drive_link) is good for y.</br>

Airbnb is an American company that operates an online marketplace for lodging, primarily homestays for vacation rentals, and tourism activities. Airbnb does not own any of the listed properties; instead, it profits by receiving commission from each booking. The company was founded in 2008. Airbnb is a shortened version of its original name, AirBedandBreakfast.com.</br>

Since 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in New York City. The following Airbnb activity is included in this New York dataset:
* Listings, including full descriptions and average review score Reviews
* Including unique id for each reviewer and detailed comments Calendar
* Including listing id and the price and availability for that day

# **1. Data Overview**
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import plotly.express as px
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/pandas/ex/Airbnb_Data/Airbnb_Open_Data.csv')
df.head()

pd.set_option('display.max_columns',500)
pd.set_option('display.max_row',500)
print(df.shape)
df.head()

"""The description about data - [Here](https://docs.google.com/spreadsheets/d/1b_dvmyhb_kAJhUmv81rAxl4KcXn0Pymz) is file with full description of data

| Name col     | Details |
| ----------- | ----------- |
| host_id      | A unique ID for house owner. When you register about the apartment rental in Airbnb, this company will assign you an ID.   |
| country   | Region of your house        |
| host_identity_verified   | The owner has been identified, hasn't ? or virtual business address     |
| minimum nights   | The minimum nights that the owner allow to hire       |
| license   | Seem to business License      |
| last review   | Last reviewed date      |
"""

df.info()

df.describe()

df.describe().transpose()

"""# **2. Clean Data**

The analyzed data above shows that we see it unreasonably in some columns. For example, in **minimum nights**, it has negative numbers.

We use this table to evaluate the meaning of cloumns. To clarify about the number of null data, we continue :
"""

df.isnull().sum().sort_values(ascending = False)

"""**Through data:**

1.   Deal with column `last review`
2.   `host_identity_verified` has null values => we will change them from " unconfirmed "
3.   Handle the column `minimum nights,availability 365` (has negative values),  `min=-1223` `max=5645`
4.   Tackle the column `price`

**The computation of median value**


* When do we use the mean value instead of median value ?
      - Need to use all information of data
      - Data has symmetrical distribution
      - Data doesn't have outliers
      - The data about quantitative, weight, height... and have continuity, stability
      - Need to compute indicators based on mean, aim to use all data
      - When data is clean and big size

#### a.   First, deal with `last review`

Transform the `last review` into the type of datetime
( khi tớ check thấy tới năm 2058 luôn =))) )

We should transform all date value in data
"""

df['last review'] = pd.to_datetime(df['last review'])
df['last review'].min(), df['last review'].max()

"""#### After replace values that is bigger than `2020`, we will check null values.
`last review` is the column containing information about the last review date or the finish review of one product or any category.
This column usually includes datetime values and indicates the time of last review or last check that was done.

We can see many rows have value in `number of reviews` but it doesn't have value in `last review`. This is null value so we will exchange them into the median value.
"""

year_value =df[df['last review'].apply(lambda x: x.year> 2022)]
print(len(year_value))

median_value = df['last review'].median()
print(median_value)

df.loc[year_value.index,'last review'] = median_value

null_value = df[(df['last review'].isnull()) & (df['number of reviews']>0)]
print(len(null_value))

df.loc[null_value.index, 'last_review'] = median_value

"""#### b. Secondly, handle `host_identity_verified` with null values
### The difference between [ df [ 'service ' ] ] and df [ 'service ' ]
- df [ 'service ' ] : is query to a column in dataframe, specifically `service`. The result returns a Series that contains all values of the column `service`.</br>
- df [ df [ 'service ' ] ] : is filter all rows, all values in column `service` of dataframe.

Now, we will find the null values and exchange them into `unconfirmed`
"""

# null_value_host = df[df['host_identity_verified'].isnull()]
# print(len(null_value_host))

all_value = df['host_identity_verified'].value_counts(dropna = False)
print(all_value)
print()

df['host_identity_verified'].fillna('unconfirmed', inplace = True)

value_after = df['host_identity_verified'].value_counts(dropna = False)
print(value_after)

"""#### c.  Third, Process the column `minimum nights` and `availability 365`
In two columns, we found some unreasonable negative values so we will multiply them with ' -1 ' to change them into positive value.

"""

df['minimum nights'] = df['minimum nights'].apply(lambda x : x*-1 if x<0 else x)
df['availability 365'] = df['availability 365'].apply(lambda x : x*-1 if x<0 else x)

"""The column `minimun nights` has some values quite large, as 399,452.. but while in `availability 365`, the max value only has 365 days. We see it unreasonably. In `availability 365` also has the same wrong values as `minimun nights`. These are outliers.

When we drop the outlier values, the cutpoint is used to define outlier values unwantedly. The values crossed the cutpoint that is usually considered as outlier values and could be dropped or dealed in another way.

We will define the cutpoint based on the quantile of 99%. Values not within or larger than this range may be considered as outliers.
"""

cutpoint = df['minimum nights'].quantile(0.99)
print('Your cutpoint: ',cutpoint)

df = df[(df['minimum nights']<=cutpoint)]

cutpoint_365 = df['availability 365'].quantile(0.99)
df = df[df['availability 365'] <= cutpoint]

"""# 4.Continue with the column  `price` and `service fee`
Both 2 columns have special signs as "$". When the column has special signs, pandas will change them into string
And more rows in columns have high value as "$1,052",....</br>
They also contain many dots " , " so we need to drop all them to change string into integer</br>
* Filter all null values, such as : 'nan, NaN, nAn,...' including tempt values. After that, we change all them into 0 to examine easily.
* Remove the special values of '$' and ','
* Convert the string to integer
=> We use this process to apply for 2 columns.


"""

df = df[~df['price'].isnull()]
df['price'] = df['price'].apply(lambda x: '0' if str(x).lower() == 'nan' else str(x))
df['price'] = df['price'].apply(lambda x: '0' if str(x).strip() == '' else str(x))

# df['price'] = df['price'].apply(lambda x : str(x).replace('$','') if '$' in str(x) else str(x))
# df['price'] = df['price'].apply(lambda x : str(x).replace(',','') if ',' in x else x)
# hay
df['price'] = df['price'].str.replace('$', '')
df['price'] = df['price'].str.replace(',', '')
df['price'] = df['price'].astype(int)

df3 = df[~df['service fee'].isnull()]
df['service fee'] = df['service fee'].astype(str)
df['service fee'] = df['service fee'].apply(lambda x: '0' if str(x).lower() == 'nan' else str(x))
df['service fee'] = df['service fee'].apply(lambda x: '0' if str(x).strip() == '' else str(x))
# df['service fee'] == df['service fee'].apply(lambda x : '0' if str(x).lower() in ['nan','none',''] else str(x))
df['service fee'] = df['service fee'].apply(lambda x : str(x).replace('$','') if '$' in str(x) else str(x))
df['service fee'] = df['service fee'].astype(int)
df.head(6)

"""# **3.Analyze**

### Neighbourhood group - barchart
"""

print(df['neighbourhood'].value_counts())
print('-' * 25)
print(df['neighbourhood'].value_counts(normalize=True))

import  matplotlib
import plotly.express as px

rm = df['neighbourhood'].value_counts()

fig = px.bar(y = rm.values,
             x = rm.index,
             color = rm.index,
             color_discrete_sequence=px.colors.sequential.Plasma,
             text = rm.values,
             title = "Neighbourhoods",
             template = "plotly_dark"
             )

fig.update_layout(
    xaxis_title = "Neighbourhood",
    yaxis_title = "count",
    font = dict(size=17,family="Franklin Gothic"),
    height=1000
    )

fig.show()

"""This chart analysis based on the zoned region. As we can see, the neighborhoods of Williamsburg and Bedford-Stuyvesant that have the numbers of homestay high are extremely. This indicates that in New York, this company mobilizes a large number of customers who are letting their property. The total quantity is focused on high the Williamsburg and Bedford-Stuyvesant areas.</br>


Through that, we suggest that the Airbnb company has a good campaign to promote their revenue. However, It seems to be distributed unevenly and less than to the right . </br>
The right skew shows that the management just focus in the Williamsburg and Bedford-Stuyvesant areas but neglect other areas, leaving them severely deficient.

### Scatter Mapbox
"""

print(df['neighbourhood group'].value_counts())

fig = px.scatter_mapbox(df,
           lat="lat",
           lon="long",
           opacity = 0.3,
           hover_name="neighbourhood group",
           hover_data=["neighbourhood group", "price"],
           color="price",
           color_discrete_sequence=px.colors.sequential.PuBuGn,
           zoom=10
           )

fig.update_layout(mapbox_style="open-street-map") # "carto-positron"
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0},
                  font = dict(size=17,family="Franklin Gothic"))
fig.show()

"""We use this chart to compare the price rely on the neighborhood groups. With longitude and latitude, we distribute them on a real openstreet map to analyze easily. The chart clearly shows the prices clearly at each address and each street. </br>
As we can see, in the Brooklyn, Manhattan and Queens neighborhood groups, the distribution is largest, densest and most even with a wide range of price levels from low to high. However, further away from these areas, the distribution becomes more discrete, uneven, and less abundant.
"""

print(df['room type'].value_counts())

"""## Piechart - Construction Year"""

print(df['Construction year'].value_counts())

constr_year = df["Construction year"].value_counts()
fig = px.pie(
    values = constr_year.values,
    names = constr_year.index,
    color_discrete_sequence = ['#CDEDDD', '#EF553B', '#00CC96', '#AB63FA'],
    title= 'Construction Year',template='plotly_dark'
)
fig.update_layout(
    font=dict(size=20,family="Franklin Gothic"),
    height=900)
fig.show()

"""## Line Chart"""

print(df['room type'].value_counts())
print('-' * 25)
print(df['host_identity_verified'].value_counts())

constr_year = df['Construction year'].value_counts().sort_index()
fig = px.line( x=constr_year.index,
              y= constr_year.values,
              markers=True,
              line_shape='linear',
              text = constr_year.values,
              height=900)

fig.update_layout(title='Construction_Year',
                  xaxis_title='Count',
                  yaxis_title='Year',
                  font = dict(size=17, family="Franklin Gothic"),
                  )
fig.show()